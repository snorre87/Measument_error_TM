{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snorre/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/snorre/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "class2english = {'kunst _musik_museum_kulturpol': 'Culture', ' klima_ miljø _ naturen ': 'Environment',\n",
    "                 'undervis_folkeskole_ungdomsuddan': 'Education', 'udenrig_ forsvaret _militær': 'Foreign affairs',\n",
    "                 ' EU _ eu': 'EU', ' sundhed': 'Health',\n",
    "                 'indvandr_flygt_udlænding': 'Immigration', ' beskæftig': 'Employment', 'skat': 'Taxes', \n",
    "                 'forskning_ universitet_videregående udd': 'Science',\n",
    "                 ' religi_ værdipol': 'Religion', ' økonom_ vækst': 'Economy', \n",
    "                 'kriminal_kriminel': 'Crime', 'hjemmehjælp_ ældre _ pension_ plejehjem': 'Elder care',\n",
    "                 'fagttig_socialt udsat_socialminis_svageste': 'Social policy'}\n",
    "def get_dic_name(num):\n",
    "    return class2english[class2num[int(num)]]\n",
    "import pickle,json,networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(29998, 'læhegn'),\n",
       "  (29999, 'heder'),\n",
       "  (30000, 'americans'),\n",
       "  (30001, 'makeamericagreatagain')],\n",
       " 30000,\n",
       " 46209)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = '/home/snorre/Dropbox/Forskning/bigdata/data_inspections/'\n",
    "meta_path = base+'simulation_results/metrics/'\n",
    "meta_path2 = base+'simulation_results/distributions/'\n",
    "from os import listdir\n",
    "result_path = '/mnt/b0c8e396-e5ba-4614-be6f-146c4c861ab3/data/topic_model_simulation/models/'\n",
    "model_runs = set(['_'.join(i.split('_')[0:3]) for i in listdir(result_path)])\n",
    "model_runs = sorted(model_runs,key=lambda x: int(x.split('_')[-1]))\n",
    "print(len(model_runs))\n",
    "#pol_df = pd.read_csv(base+'data/topic_modelling_dataset.csv').drop('bow',axis=1)\n",
    "import pickle\n",
    "\n",
    "pol_df,noise_df, Index = pickle.load(open(base+'topic_simulation_dependencies.pkl','rb'))\n",
    "# Define where the noise begins.\n",
    "noise_start_w = 30000\n",
    "noise_start_d = len(pol_df)\n",
    "list(enumerate(Index))[noise_start_w-2:noise_start_w+2],noise_start_w,noise_start_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46209, 32400)\n",
      "(46209, 32400)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "url_pattern = '(?:(?:https?:\\/\\/|www\\.)\\w+[\\.\\w]*\\.\\w+)|(?:\\w+\\.(?:com|dk))|(?:https?://|www\\.)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "url_re = re.compile(url_pattern)\n",
    "url_sub = ' __url__ '\n",
    "digit_pattern = r\"\\d(?:[\\.,\\d]*\\d+)|\\d{2,}(?:[\\.,\\d]*\\d+)?\" # matching >1 figure digits\n",
    "replace_regex = [(url_sub,url_re),(' __digit__ ',re.compile(digit_pattern))]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def filter_words(words,stopwords):\n",
    "    # filter non signs\n",
    "    words = [word for word in words if isalphanum(word)]\n",
    "    # filter stopwords\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    return words\n",
    "def isalphanum(string):\n",
    "    if not string.isalnum():\n",
    "        # find word with - between\n",
    "        pattern = r'[a-zAZøæåÆØÅ]+\\-[a-zAZøæåÆØÅ]*|__[a-zAZøæåÆØÅ]+_*'\n",
    "        search = re.findall(pattern,string)\n",
    "        if len(search)>0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "def process_documents(text,stopwords):\n",
    "    # replace known patterns.\n",
    "    for sub_pattern,regex in replace_regex:\n",
    "        text = regex.sub(sub_pattern,text)\n",
    "    text = nltk.tokenize.word_tokenize(text)\n",
    "    # lower\n",
    "    text = [i.lower() for i in text]\n",
    "    # remove stopwords\n",
    "    text = filter_words(text,stopwords)\n",
    "    return text\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english')) | set(['__digit__'])# english # 'hillary','trump'\n",
    "stopwords = set(nltk.corpus.stopwords.words('danish'))#|set(open('danish_stopwords.txt','r').read().split('\\n'))|set(['__digit__']) # danish\n",
    "\n",
    "#noise_docs = noise_df.message.apply(process_documents,**{'stopwords':stopWords}).values\n",
    "real_docs = pol_df.message.apply(process_documents,**{'stopwords':stopwords}).values\n",
    "\n",
    "def to_dense(corpus,vocab_size):\n",
    "    X = np.zeros((len(corpus),vocab_size),dtype=np.int32)\n",
    "    for num in range(len(corpus)):\n",
    "        bow = corpus[num]\n",
    "        for w,count in bow.items():\n",
    "            try:\n",
    "                X[num][w]=count\n",
    "            except:\n",
    "                pass\n",
    "    return X\n",
    "import scipy.sparse as sp\n",
    "def to_sparse(corpus,vocab_size):\n",
    "    X = sp.dok_matrix((len(corpus),vocab_size), dtype=np.int32)\n",
    "    for num in range(len(corpus)):\n",
    "        bow = corpus[num]\n",
    "        for w,count in bow.items():            \n",
    "            X[num,w]=count\n",
    "    print(X.shape)\n",
    "    X = X.tocsr()\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "def to_index(text,d):\n",
    "    return [d[i] for i in text if i in d]\n",
    "def to_bow(texts,d,sparse = True):\n",
    "    global vocab_size\n",
    "    bows = [Counter(to_index(text,d)) for text in texts]\n",
    "    if sparse:\n",
    "        return to_sparse(bows,vocab_size)\n",
    "    else:\n",
    "        return bows\n",
    "vocab_size = len(Index)\n",
    "w2id = {w:num for num,w in enumerate(Index)}\n",
    "real_bows = to_bow(real_docs,w2id)\n",
    "#noise_bows = to_bow(noise_docs,w2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_bows = real_bows.toarray()[:,0:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "class LLDA:\n",
    "    def __init__(self, K, alpha, beta):\n",
    "        #self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def term_to_id(self, term):\n",
    "        if term not in self.vocas_id:\n",
    "            voca_id = len(self.vocas)\n",
    "            self.vocas_id[term] = voca_id\n",
    "            self.vocas.append(term)\n",
    "        else:\n",
    "            voca_id = self.vocas_id[term]\n",
    "        return voca_id\n",
    "\n",
    "    def complement_label(self, label):\n",
    "        if not label: return numpy.ones(len(self.labelmap))\n",
    "        vec = numpy.zeros(len(self.labelmap))\n",
    "        vec[0] = 1.0\n",
    "        for x in label: vec[self.labelmap[x]] = 1.0\n",
    "        return vec\n",
    "\n",
    "    def set_corpus(self, labelset, corpus, labels):\n",
    "        labelset.insert(0, \"common\")\n",
    "        self.labelmap = dict(zip(labelset, range(len(labelset))))\n",
    "        self.K = len(self.labelmap)\n",
    "\n",
    "        self.vocas = []\n",
    "        self.vocas_id = dict()\n",
    "        self.labels = numpy.array([self.complement_label(label) for label in labels])\n",
    "        self.docs = [[self.term_to_id(term) for term in doc] for doc in corpus]\n",
    "\n",
    "        M = len(corpus)\n",
    "        V = len(self.vocas)\n",
    "\n",
    "        self.z_m_n = []\n",
    "        self.n_m_z = numpy.zeros((M, self.K), dtype=int)\n",
    "        self.n_z_t = numpy.zeros((self.K, V), dtype=int)\n",
    "        self.n_z = numpy.zeros(self.K, dtype=int)\n",
    "\n",
    "        for m, doc, label in tqdm(zip(range(M), self.docs, self.labels)):\n",
    "            N_m = len(doc)\n",
    "            #z_n = [label[x] for x in numpy.random.randint(len(label), size=N_m)]\n",
    "            z_n = [numpy.random.multinomial(1, label / label.sum()).argmax() for x in range(N_m)]\n",
    "            self.z_m_n.append(z_n)\n",
    "            for t, z in zip(doc, z_n):\n",
    "                self.n_m_z[m, z] += 1\n",
    "                self.n_z_t[z, t] += 1\n",
    "                self.n_z[z] += 1\n",
    "\n",
    "    def inference(self):\n",
    "        V = len(self.vocas)\n",
    "        for m, doc, label in zip(range(len(self.docs)), self.docs, self.labels):\n",
    "            for n in range(len(doc)):\n",
    "                t = doc[n]\n",
    "                z = self.z_m_n[m][n]\n",
    "                self.n_m_z[m, z] -= 1\n",
    "                self.n_z_t[z, t] -= 1\n",
    "                self.n_z[z] -= 1\n",
    "\n",
    "                denom_a = self.n_m_z[m].sum() + self.K * self.alpha\n",
    "                denom_b = self.n_z_t.sum(axis=1) + V * self.beta\n",
    "                p_z = label * (self.n_z_t[:, t] + self.beta) / denom_b * (self.n_m_z[m] + self.alpha) / denom_a\n",
    "                new_z = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()\n",
    "\n",
    "                self.z_m_n[m][n] = new_z\n",
    "                self.n_m_z[m, new_z] += 1\n",
    "                self.n_z_t[new_z, t] += 1\n",
    "                self.n_z[new_z] += 1\n",
    "\n",
    "    def phi(self):\n",
    "        V = len(self.vocas)\n",
    "        return (self.n_z_t + self.beta) / (self.n_z[:, numpy.newaxis] + V * self.beta)\n",
    "\n",
    "    def theta(self):\n",
    "        \"\"\"document-topic distribution\"\"\"\n",
    "        n_alpha = self.n_m_z + self.labels * self.alpha\n",
    "        return n_alpha / n_alpha.sum(axis=1)[:, numpy.newaxis]\n",
    "\n",
    "    def perplexity(self, docs=None):\n",
    "        if docs == None: docs = self.docs\n",
    "        phi = self.phi()\n",
    "        thetas = self.theta()\n",
    "\n",
    "        log_per = N = 0\n",
    "        for doc, theta in zip(docs, thetas):\n",
    "            for w in doc:\n",
    "                log_per -= numpy.log(numpy.inner(phi[:,w], theta))\n",
    "            N += len(doc)\n",
    "        return numpy.exp(log_per / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class2english = {'kunst _musik_museum_kulturpol': 'Culture', ' klima_ miljø _ naturen ': 'Environment',\n",
    "                 'undervis_folkeskole_ungdomsuddan': 'Education', 'udenrig_ forsvaret _militær': 'Foreign affairs',\n",
    "                 ' EU _ eu': 'EU', ' sundhed': 'Health',\n",
    "                 'indvandr_flygt_udlænding': 'Immigration', ' beskæftig': 'Employment', 'skat': 'Taxes', \n",
    "                 'forskning_ universitet_videregående udd': 'Science',\n",
    "                 ' religi_ værdipol': 'Religion', ' økonom_ vækst': 'Economy', \n",
    "                 'kriminal_kriminel': 'Crime', 'hjemmehjælp_ ældre _ pension_ plejehjem': 'Elder care',\n",
    "                 'fattig_socialt udsat_socialminis_svageste': 'Social policy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class2num = dict(pol_df[['topic','topic_class']].values)\n",
    "class2num = sorted(class2num,key=lambda x: class2num[x])\n",
    "\n",
    "def get_dic_name(num):\n",
    "    return class2english[class2num[int(num)]]\n",
    "labels = pol_df.topic_class.apply(get_dic_name).values\n",
    "labelset = list(set(labels))\n",
    "\n",
    "corpus = real_bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_sequential_doc(bow):\n",
    "    return np.arange(len(bow))[bow]\n",
    "corpus = [to_sequential_doc(bow) for bow in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_docs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pol_df.topic_class.apply(get_dic_name).values[0:n_docs]\n",
    "labelset = list(set(labels))\n",
    "temp_corpus = corpus[0:n_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4874152fa248a1b39ba14e33c1f65f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "llda = LLDA(15, 1/15, 1/len(Index))\n",
    "llda.set_corpus(labelset, temp_corpus, labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfe406632544429b384b81ba37c95b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "    #print(\"-- %d \" % (i + 1))\n",
    "    llda.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
